model_arch: Transformer_EGNN_v2
depth: 5
ascend_depth: 5
num_heads: 2
num_ascend_heads: 2
pool: "add"
sub_aggr: "mean"
only_ground: True
residual: True
norm: "layer"
mask: True
trainer: qm9
learning_rate: 0.0002
RFF_dim: null
RFF_sigma: null
epochs: 1600
warmup_epochs: 10
batch_size: 96
fully_connect: True
subgraph_dict:
  mode: "transformer_10"
node_features: 21
LABEL_INDEX: 7
edge_features: 0
hidden_features: 48
out_features: 1
data_dir: "./data/qm9"
model_dir: "./trained/qm9"
log_dir: "./logs"
optimizer:
  name: Adam
  kwargs:
    betas: [0.9, 0.999]
    eps: 1e-8
criterion:
  name: L1Loss
scheduler:
  name: CosineAnnealingLR
  kwargs:
    T_max: ${epochs}
device: cuda
