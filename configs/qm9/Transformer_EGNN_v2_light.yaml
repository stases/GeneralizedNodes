model_arch: Transformer_EGNN_v2
depth: 2
ascend_depth: 0
num_heads: 2
num_ascend_heads: 0
pool: "add"
sub_aggr: "add"
only_ground: False
only_sub: True
residual: True
norm: "layer"
mask: True
trainer: qm9
learning_rate: 0.03
RFF_dim: null
RFF_sigma: null
epochs: 11
warmup_epochs: 50
batch_size: 32
fully_connect: True
subgraph_dict:
  mode: "transformer_6"
node_features: 17
LABEL_INDEX: 7
edge_features: 0
hidden_features: 64
out_features: 1
data_dir: "./data/qm9"
model_dir: "./trained/qm9"
log_dir: "./logs"
optimizer:
  name: Adam
  kwargs:
    betas: [0.9, 0.999]
    eps: 1e-8
criterion:
  name: L1Loss
scheduler:
  name: CosineAnnealingLR
  kwargs:
    T_max: ${epochs}
device: cuda
