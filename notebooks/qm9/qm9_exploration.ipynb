{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-19T11:55:48.086256362Z",
     "start_time": "2023-08-19T11:55:43.662005871Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "qm9 = QM9(root='../data', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as tg\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class EGNNLayer(tg.nn.MessagePassing):\n",
    "    def __init__(self, emb_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"E(n) Equivariant GNN Layer\n",
    "\n",
    "        Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "\n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": nn.SiLU(), \"relu\": nn.ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = nn.Sequential(\n",
    "            nn.Linear(2 * emb_dim + 1, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\psi_x` for computing messages `\\overrightarrow{m}_ij`\n",
    "        self.mlp_pos = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim), self.norm(emb_dim), self.activation, nn.Linear(emb_dim, 1)\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = nn.Sequential(\n",
    "            nn.Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, pos_i, pos_j):\n",
    "        # Compute messages\n",
    "        pos_diff = pos_i - pos_j\n",
    "        dists = torch.norm(pos_diff, dim=-1).unsqueeze(1)\n",
    "        msg = torch.cat([h_i, h_j, dists], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        # Scale magnitude of displacement vector\n",
    "        pos_diff = pos_diff * self.mlp_pos(msg)  # torch.clamp(updates, min=-100, max=100)\n",
    "        return msg, pos_diff\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        msgs, pos_diffs = inputs\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(msgs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        pos_aggr = scatter(pos_diffs, index, dim=self.node_dim, reduce=\"mean\")\n",
    "        return msg_aggr, pos_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, pos):\n",
    "        msg_aggr, pos_aggr = aggr_out\n",
    "        upd_out = self.mlp_upd(torch.cat([h, msg_aggr], dim=-1))\n",
    "        upd_pos = pos + pos_aggr\n",
    "        return upd_out, upd_pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\"\n",
    "class EGNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            depth=5,\n",
    "            hidden_features=128,\n",
    "            node_features=1,\n",
    "            out_features=1,\n",
    "            activation=\"relu\",\n",
    "            norm=\"layer\",\n",
    "            aggr=\"sum\",\n",
    "            pool=\"add\",\n",
    "            residual=True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model\n",
    "\n",
    "        Args:\n",
    "            depth: (int) - number of message passing layers\n",
    "            hidden_features: (int) - hidden dimension\n",
    "            node_features: (int) - initial node feature dimension\n",
    "            out_features: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Name of the network\n",
    "        self.name = \"EGNN\"\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(node_features, hidden_features)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(depth):\n",
    "            self.convs.append(EGNNLayer(hidden_features, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": tg.nn.global_mean_pool, \"add\": tg.nn.global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.pred = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, hidden_features),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_features, out_features)\n",
    "        )\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update\n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T11:55:50.074599732Z",
     "start_time": "2023-08-19T11:55:50.055113376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "datapoint = qm9[0]\n",
    "train = qm9[:1000]\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "model = EGNN(depth=5, hidden_features=128, node_features=11, out_features=1, activation=\"relu\", norm=\"layer\", aggr=\"sum\", pool=\"add\", residual=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T22:14:29.143543668Z",
     "start_time": "2023-08-16T22:14:29.106772200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# get a single batch\n",
    "batch = next(iter(train_loader))\n",
    "out = model(batch)\n",
    "# check if it requires gradient\n",
    "print(out.requires_grad)\n",
    "# print out shape\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T22:14:29.594035369Z",
     "start_time": "2023-08-16T22:14:29.505690841Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoint"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T22:14:47.422551198Z",
     "start_time": "2023-08-16T22:14:47.417936477Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1 = QM9(root='../data', transform=None)\n",
    "datapoint_1 = dataset_1[0]\n",
    "datapoint_1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T11:56:25.567267929Z",
     "start_time": "2023-08-19T11:56:25.372780330Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[14, 11], edge_index=[2, 28], edge_attr=[28, 4], y=[1, 19], pos=[14, 3], idx=[1], name='gdb_29153', z=[14])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2 = QM9(root='../data', transform=None)\n",
    "dataset_2 = dataset_2.shuffle()\n",
    "datapoint_2 = dataset_2[0]\n",
    "datapoint_2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T11:56:41.601567717Z",
     "start_time": "2023-08-19T11:56:41.412278264Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
